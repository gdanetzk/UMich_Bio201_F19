---
title: 'Bio 201: Lab 3 tidy data'
author: "Kristi Gdanetz MacCready"
date: "9/23/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "~/Documents/UMich_Bio201_F19/Lab3/")
```

# Load packages
Packages in R are basically sets of additional functions that let you do more stuff. The functions you’ve been using so far are part of R (also called base install) and no extra action is needed to utilize these functions; packages give you access to more functions. Before you use a package for the first time you need to install it on your machine. Then any subsequent R sessions where the package will be used require an import step at the beginning of the session. The tidyverse package should already be installed on the USB lab computers. This is an “umbrella-package” that installs several packages useful for data analysis which work together well such as tidyr, dplyr, ggplot2, tibble, etc.
 
The tidyverse package tries to address 3 common issues that arise when doing data analysis with some of the functions that come with R:
 
The results from a base R function sometimes depend on the type of data.
Using R expressions in a non standard way, which can be confusing for new learners.
Hidden arguments, having default operations that new learners are not aware of.

There are several other useful packages that are part of the tidyverse or are written to work well with the tidyverse. Install and load these packages: readxl, broom, cowplot. 

```{r Load packages, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
library(tidyverse)
library(readxl)
set.seed(7)
```

# Loading data

There are several methods to read in the SCFA measurement data. We could use the readr() package, which is loaded as part of the tidyverse. This package has functions for reading in specific file formats; general delimited files (read_delim), tab separated values (read_tsv), comma separated values files (read_csv), fixed width files (read_fwf), and files where columns are separated by whitespace (read_table). We are also going to use the read_excel function from the readxl package to read a table in from a Microsoft Excel-formatted spreadsheet since Excel spreadsheets are a common way collaborates share data.  
 
Each of these functions has a decent number of options that default to values that are generally intuitive. Be careful - there are other similarly named functions (e.g. read.tsv) that are actually part of base R and have somewhat unexpected defaults. What are the defaults of these functions? What options can you change? Remember from the last lesson that you can view the documentation for any function. 

### with readr
The output of the read functions that are part of the tidyverse are a special type of data frame called a tibble. To back up a step, what is a data frame? A data frame can be thought of as a table where each row represents a different entity and each column represents a different aspect of that entity. For example, the scfa_wkly variable stores the value of a data frame where each row represents a different person and each column represents various attributes of those people such as their participant identification number, weight, height, location, diagnosis, smoking status, etc. Each row has the same number of columns. If a piece of data is missing, then R will denote the value for that entity with the NA value. Got it? In summary, a tibble is a special type of data frame that is a stripped down version of the data.frame structure that is core to R. Keeping with the . for _ theme, data_frame can be used as an alias for tibble.
 
There are some special aspects of a tibble to be aware of. Perhaps most important is that there are no names on the rows. Absence of row names is a safety measure to protect you from some weird quirks in R. Another difference is when you enter the name of the data frame at the prompt, instead of having the entire data frame vomited at your screen, you get an abbreviated output.

```{r}
scfa_wkly <- read_delim(file = "raw_data/SCFA_wkly.txt", 
                        delim = "\t", escape_double = FALSE, trim_ws = TRUE, na=c("NA"),
                        col_types = list())
``` 



```{r}
scfa_wkly

rm(scfa_wkly)
```

The abbreviated output gives the first ten columns and the first ten rows of the data frame. You’ll notice that at the top of the output, it tells us that there are X rows and X columns. The column headings for the X columns that weren’t outputted are listed at the bottom of the output. It also indicates that X rows were not included in the output. In addition, the output tells us what type of variable each column contains. For example, the tube_wt column contains dbl or double precision numbers and the Participant_ID column contains chr or character values. You’ll also notice that zero values have a lighter color and that any NAs are red. These features are all meant to improve the visualization of the data.

This is referred to as a “tibble”. Tibbles tweak some of the behaviors of the data frame objects you introduced last week. The data structure is very similar to a data frame. For our purposes the only differences are that:

* In addition to displaying the data type of each column under its name, it only prints the first few rows of data and only as many columns as fit on one screen.
* Columns of class character are never converted into factors.

### Import with excel files
```{r}
scfa_indv <- read_excel("raw_data/SCFA_data.xlsx",
                        sheet = "SCFA_indv", col_names = TRUE, trim_ws = TRUE, 
                        na = c("", "NA", "-----")) 
```


# Exploring data

Let’s dig into the data to think about how we’d like to use it to answer our research questions. Whenever you read in a data frame, there are a few things to complete to get a handle on your data. First, as we’ve already done, entering the name of the data frame at the prompt will tell us a lot of information. We might also want to access individual chunks of those data. 

### Summary functions
```{r eval=FALSE, include=FALSE}
nrow()
ncol()
dim()
colnames()
rownames()
glimpse()
str()
```


### Acessor functions

Take a moment to look at the columns represented in the data frame and the information presented below the column names. Do all of the values seem reasonable? Need a hint? Check out the information below “Height” and “Weight”. Think someone could weigh 0 kg or stand 0 cm tall? To view and manipulate an individual column there are three options:

```{r}
summary(scfa_indv)

select(scfa_indv, "ht_in")
scfa_indv$ht_in
scfa_indv[["ht_in"]]
scfa_indv[[5]]
```

The first three options are pretty solid, the last option is a bit of a hassle since it requires us to count columns. Whichever approach you select, stick with it and be consistent in your coding. We will primarily use the first and second approaches. These commands pull out a column from the data frame and convert it to a vector. We can use the <- operator to update our columns. The dplyr package, which is one of the core packages within the tidyverse, has a useful function called na_if. If it finds the value specified in the vector, it will convert it to an NA.

```{r}
scfa_indv$Age <- na_if(scfa_indv$Age, 0)
```

Running summary(scfa_indv) again, you see that the range for the “Age” column is more reasonable now. We’d like to look at the values for our columns that contain character values, but they’re obfuscated. One way to check this out is with the count command

```{r}
count(scfa_indv, Supplement_consumed)
```

Notice anything weird here? Yup. In the “Supplement_consumed” column, it looks like formatting for supplement mixes were inconsistent (some used + and other used &). We can use the dplyr function recode to make this easy...

```{r}
scfa_indv$Supplement_consumed <- replace_na(scfa_indv$Supplement_consumed, "none")

scfa_indv[["Supplement_consumed"]] <- recode(.x=scfa_indv[["Supplement_consumed"]], "BRMPS&Accessible"="BRMPS+Accessible")
```

### Column names

Look back at the column names in the scfa_wkly data frame, notice that some names are in title case (e.g., “Participant_ID”) and others are in all lower case (e.g., “tube_wt”). Also, some of the column names may not make sense if you are not familiar with the types of SCFAs measured. Some of the column names contain spaces, while others use underscores. Let’s fix these issues to make using the data easier.

```{r}
colnames(scfa_indv)
```

There are two problems with the name “tube_wt”; the capitalization is inconsistent with the other columns and it may not be immediately clear what “tube_wt” means. We have many options for how to name things. “tube_wt” could be written as “Tube_wt”, “TubeWt”, “tube.wt”, “tube_weight”, etc. Additionally, when was this weight collected; before or after the sample was added to the tube? 

```{r}
scfa_indv <- rename(scfa_indv, sample_wt_g = tube_wt)
```

The general preference in the R world is to use lowercase lettering and to separate words in a name with an underscore (i.e. _). This is called “snake case”. Having a consistent capitalization strategy may seem a bit pedantic, but it makes it easier to keep the names straight when you don’t have to remember capitalization. We can convert the column names to lower case using the rename_all() function in the dplyr package with the tolower() function. Conversely, if you wanted everything in all caps, you could use the toupper() function.

Use the rename function in the dplyr package to rename individual column names, similar to using the recode function to correct the data entry typos. 

```{r}
scfa_indv <- rename_all(scfa_indv, tolower)
```

Before procedding, consider adding units to some of our columns. For example, you might rename the “sample_weight” column to “sample_weight_g”. Some of the modifications to column headings discussed above are a matter of personal preference. At the end of the day consistency is most important; it makes your analysis easier to implement and share with collaborators. 

This is especially true if you have to pause the project for a few weeks or months (e.g., you go on vacation, the paper goes out for review, etc.). When you come back to it, you won’t have to recall what the abbreviated columns names represent. 

```{r}
scfa_indv <- rename(scfa_indv,
                    acetate_mM = ace, 
                    butyrate_mM = but,
                    propionate_mM = pro)
```

Making sure the values in the data frame are correct by removing typos and ensuring they are properly bounded (e.g., no weights of zero, or concentrations 100x the biological limit) is critical to the validity of any analysis. 

# dplyr + tidyr

As introduced in the previous section, bracket subsetting can be cumbersome and difficult to read, especially for complicated operations. Enter dplyr. dplyr is a package for making tabular data manipulation easier. It pairs nicely with tidyr which enables you to swiftly convert between different data formats for plotting and analysis. We’re going to learn some of the most common dplyr functions:

```{r eval=FALSE, include=FALSE}
select() #subset columns
filter() #subset rows on conditions
mutate() #create new columns by using information from other columns
group_by() and summarize() #create summary statistics of grouped data
arrange() #sort results
count() #count discrete values
```

### selecting columns

The first argument to this function is the data frame (scfa_wkly), and the subsequent arguments are the columns to keep. To retain columns of a data frame, use select(): 

```{r}
select(scfa_indv, study_week, participant_id, butyrate_mM)
```

To retain all columns except certain ones, put a “-” in front of the variable to exclude it. This will select all the variables in scfa_wkly except use_data and notes:  

```{r}
select(scfa_indv, -use_data, -notes)
```

### filtering rows

To choose or exclude rows based on specific criteria, use filter(): 

```{r}
filter(scfa_indv, semester == "Fall2018") #choose matches

filter(scfa_indv, notes != "frozen >24hrs") #exclude matches
```

### pipes

What if you want to select and filter at the same time? There are three ways to do this: use intermediate steps, nested functions, pipes.

With intermediate steps, you create a temporary data frame and use that as input to the next function.
```{r}
scfa_indv2 <- filter(scfa_indv, sample_wt_g > 0.10)
but_data <- select(scfa_indv2, 
                   participant_id, study_week, supplement_consumed, butyrate_mM, notes)
```

This is readable, but can clutter up your workspace with lots of objects that you have to name and keep track of individually. 

You can also nest functions (i.e. one function inside another).

```{r}
but_data <- select(filter(scfa_indv, sample_wt_g > 0.10), 
                   participant_id, study_week, supplement_consumed, butyrate_mM, notes)
```

This is handy, but can be difficult to read if more than a couple functions are nested, as R evaluates the expression from the inside out (in this case, filtering, then selecting).

The last option, pipes, are a recent addition to R. Pipes let you take the output of one function and send it directly to the next, which is useful when you need to do many things to the same dataset. Pipes in R look like %>% and are made available via the magrittr package, installed automatically with dplyr. If you use RStudio, you can type the pipe with the shortcut: Cmd + Shift + M on a Mac (Ctrl + Shift + M if you have a PC).

```{r}
scfa_indv %>%
  filter(sample_wt_g > 0.10) %>% 
  filter(notes != "frozen >24hrs") %>% 
  select(participant_id, study_week, supplement_consumed, butyrate_mM, notes)
```

In the above code, the pipe sent the scfa_indv dataset first through filter() to retain rows where sample weight is greater than 0.10, then through select() to retain only certain columns. Since %>% takes the object on its left and passes it as the first argument to the function on its right, there is no need to explicitly include the data frame as an argument to the filter() and select() functions.

Some may find it helpful to read the pipe like the word “then”. For instance, in the above example, the data frame scfa_indv, was filtered for rows with samples with a weight > 0.10, then the columns participant_id, study_week, supplement_consumed, butyrate_mM, and notes were selected. The dplyr functions by themselves are somewhat simple, but by combining them into linear workflows with the pipe, more complex manipulations of data frames are accomplished.

### Challenge 3.1 
```{r}
scfa_indv %>%
  filter(sample_wt_g > 0.10 & sample_wt_g < 1.00) %>% 
  filter(notes != "frozen >24hrs") %>%
  select(participant_id, study_week, supplement_consumed, butyrate_mM, notes)

f18_but_rps <- scfa_indv %>%
  filter(semester == "fall2018", 
         study_week != "week4",
         supplement_consumed == "BRMPS",
         notes != "frozen >24hrs", 
         sample_wt_g > 0.10 & sample_wt_g < 1.00) %>%
  select(participant_id, study_week, supplement_consumed, butyrate_mM, -notes)
```


# calculations across columns

### mutate

Frequently you’ll want to create new columns based on the values in existing columns, for example to do unit conversions, or to find the ratio of values in two columns. For this we’ll use mutate().

To create a new column of weight in kg:
```{r}
mutate(scfa_indv, sample_kg = sample_wt_g / 1000)
```

You can also create a second new column based on the first new column within the same call of mutate():

```{r}
mutate(scfa_indv,
       sample_kg = sample_wt_g / 1000, 
       acetate_mmol_kg = (acetate_mM*0.002)/sample_kg) 
```

If this runs off your screen and you just want to see the first few rows, you can use a pipe to view the head() of the data. (Pipes work with non-dplyr functions, too, as long as the dplyr or magrittr package is loaded).

```{r}
# 1 kg = 2.205 pounds
# 1 m = 35.274 inches 
```

### Challenge 3.2
Do the following as one long series of commands using pipes:

* Rename columns as described above
* Convert measurements from US Customary to metric units 
* Round participant height and weight to 0 decimal
* Subset for samples within the weight limits 
* Subset for samples that were frozen within 24 hours
* Convert sample weights to kilograms
* Calculate mmol/kg for each SCFA
* Calculate the total SCFA
* Round all SCFA measurments to 2 decimal
* Drop intermediate columns used for calculations 

```{r}
scfa_indv_qc <- scfa_indv %>%
  rename_all(tolower) %>%
  rename(race_ethnicity = "race/ethnicity",
         sample_wt_g = tube_wt, 
         acetate_mM = ace, 
         butyrate_mM = but,
         propionate_mM = pro)%>% 
  mutate(participant_id = toupper(participant_id),
         age = na_if(age, 0),
         ht_cm = round((ht_in*2.54), digits = 0),
         wt_kg = round((wt_lbs/2.205), digits = 0)) %>%
  filter(use_data == "yes",
         notes != "frozen >24hrs", 
         sample_wt_g > 0.10 & sample_wt_g < 1.00) %>%
  mutate(sample_kg = sample_wt_g / 1000) %>%
  mutate(acetate_mmol_kg = round(((acetate_mM*0.002)/sample_kg), digits = 2),
         butyrate_mmol_kg = round(((butyrate_mM*0.002)/sample_kg), digits = 2),
         propionate_mmol_kg = round(((propionate_mM*0.002)/sample_kg), digits = 2),
         total_scfa = round((acetate_mmol_kg + butyrate_mmol_kg + propionate_mmol_kg), digits = 2)) %>%
  select(-use_data, -ht_in, -wt_lbs, -ends_with("_mM"), -sample_kg, -sample_wt_g, -notes)
```

Swap your code with your neighbor, does their code run on your computer? Are the outputs the same?


# Export data frame

After curating, extracting information, or summarising raw data, researchers often want to export these curated data sets to save them for future use or to share them with collaborators. Similar to the read_delim() function used for reading CSV or TSV files into R, there is a write_delim() function that generates files from data frames. 

Before using write_delim(), create a new folder, curated_data, in the working directory to store this curated dataset; curated datasets should not be stored in the same directory as raw data. It’s good practice to keep them separate. As stated previously, the raw_data folder should only contain the raw, unaltered data. The code in this section will generate the contents of the curated_data directory, so the files curated_data contains can be recreated if necessary. 

```{r}
write_delim(scfa_indv_qc, path = "curated_data/scfa_indv_qc.txt", delim = "\t")
```

Remember the importance of leaving the raw data raw. Our manipulations of the scfa_indv data frame have not altered raw_data/scfa_data.xlsx. Now the cleaned up data frame to is ready share with collaborators or to use in Lab 4 next week, and all the code contained in this Rmarkdown document serves as a log of the changes made to the raw data. 


-----
end